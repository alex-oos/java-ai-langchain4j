spring.application.name=java-ai-langchain4j
server.port=8080
# 默认 langchain4j
#langchain4j.open-ai.chat-model.base-url=http://langchain4j.dev/demo/openai/v1
#langchain4j.open-ai.chat-model.api-key="demo"
#langchain4j.open-ai.chat-model.model-name=gpt-4o-mini
#日志
langchain4j.open-ai.chat-model.log-requests=true
langchain4j.open-ai.chat-model.log-responses=true
# deepseek
langchain4j.open-ai.chat-model.base-url=https://api.deepseek.ai/v1
langchain4j.open-ai.chat-model.api-key=sk-83493f6b46b248c5a3a008dca639a25
# deepseek-v3
langchain4j.open-ai.chat-model.model-name=deepseek-chat
#langchain4j.community.dashscope.chat-model-name=deepseek-reasoner
# ollama
langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.api-key=sk-83493f6b46b248c5a3a008dca639ac25
langchain4j.ollama.chat-model.model-name=llama2-13b-chat
langchain4j.ollama.chat-model.log-requests=true
langchain4j.ollama.chat-model.log-responses=true
#百炼模型
langchain4j.community.dashscope.chat-model.api-key=sk-83493f6b46b248c5a3a008dca639ac25
langchain4j.community.dashscope.chat-model.model-name=qwen-max
#百炼模式中的deepseek
#langchain4j.open-ai.chat-model.base-url=https://dashscope.aliyuncs.com/compatible-mode/v1
#langchain4j.open-ai.chat-model.api-key= sk-83493f6b46b248c5a3a008dca639ac25
#langchain4j.open-ai.chat-model.model-name=deepseek-v3
#温度系数：取值范围通常在 0 到 1 之间。值越高，模型的输出越随机、富有创造性； # 值越低，输出越确定、保守。这里设置为 0.9，意味着模型会有一定的随机性，生成的回复可能会比较多样化。
langchain4j.open-ai.chat-model.temperature=0.9
# 流式配置
langchain4j.open-ai.streaming-chat-model.base-url=https://dashscope.aliyuncs.com/compatible-mode/v1
langchain4j.open-ai.streaming-chat-model.api-key=sk-83493f6b46b248c5a3a008dca639ac25
langchain4j.open-ai.streaming-chat-model.model-name=deepseek-v3
langchain4j.open-ai.streaming-chat-model.log-requests=true
langchain4j.open-ai.streaming-chat-model.log-responses=true
logging.level.root=debug
#MongoDB连接配置
spring.data.mongodb.uri=mongodb://localhost:27017/chat_memory_db
